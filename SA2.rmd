## Introduction

A recent study on American breakfast preferences found that 29% of Americanscite that toast is their go-to breakfast of choice.  Toast is also an incredibly versatile food, which can be enjoyed throughout the day.  Millennial and Gen-Z trends have driven a recent increase in the popularity of toast, with a notableaffinity for avocado and hummus toast.  A 2014 op-ed in the New Yorker titled The Trend is Toastargued that toast is so popular among young people thatit could be considered “tip of the hipster sphere”.  The rise in popularity of toast among younger generations has opened a high-potential market space tocombine  technology  with  toast.   Thus,  our  client,  Toast  Co.,  has  created  theToast-UBS© which enables individuals to curate gourmet, artisanal toast rightfrom their computer.  


Our  aim  is  to  assist  Toast  Co. to  make  informed,  data-driven  decisions  re-garding the launch of Toast-USB©.  The Design of Experiments Team (DOE) conducted an in-depth market analysis of 421 subjects from five metropolitan areas.  Preliminary results indicated a high potential for success of the product, with 50.36% of the sample responding favorably to the Toast-USB©, nearly doubling the favorable response necessary to break-even on the product.  In addition, Toast Co.  has supplied us with a second data set to be used to evaluatequantitative probability of success of the Toast-USB©.  During the course ofthis investigation, we will synthesize the key findings from the first and secondstudy, perform a risk-benefit analysis on continuation of the campaign, providea binary recommendation for launch of the product,  and outline top grossingmarket demographics, and determine an optimal Manufacture Suggested RetailPrice.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(data.table)
library(tibble)
library(GGally)
library(kableExtra)
library(ggpubr)

#Read in Data
raw_data = read_csv("prac_data.csv") %>%
  mutate(
    price = unlist(read_csv("price.csv")),
    job = as.factor(replace_na(job,"Unknown")),
    education = replace_na(education,"Unknown"),
    education = replace(education,which(education == "unknown"),"Unknown"),
    contact = replace_na(contact,"Unknown"),
    contact = replace(contact,which(contact == "unknown"),"Unknown"),
    pdays = replace_na(pdays,-1),
    default = replace_na(default,"Unknown"),
    marital = replace_na(marital,"Unkown"),
    housing = replace_na(housing,"Unkown"),
    loan = replace_na(loan,"Unkown"),
    previous = replace_na(previous,0),
    poutcome = replace_na(poutcome,"Unknown"),
    inPrevious = if_else(previous > 0,TRUE,FALSE),
    numMissing = rowSums(across(everything(), ~is.na(.)))
  )
numericColumns = c(1,6,10,12,13,14,15,18)
categoricalColumns = c(2,3,4,5,7,8,9)
df_colnames = colnames(raw_data)

proportion_df = raw_data %>%
  group_by(job) %>%
  summarize(n = n())

mean_df = raw_data %>%
  summarize(across(numericColumns,function(x) round(mean(x, na.rm = TRUE),2))) %>%
  pivot_longer(everything(), names_to = "Variable",values_to = "mean") %>%
  na.omit()
sd_df = raw_data %>%
  summarize(across(numericColumns,function(x) round(sd(x, na.rm = TRUE),2))) %>%
  pivot_longer(everything(),names_to = "Variable",values_to = "sd") %>%
  na.omit()
median_df = raw_data %>%
  summarize(across(numericColumns,function(x) median(x,na.rm = TRUE))) %>%
  pivot_longer(everything(),names_to = "Variable", values_to = "median") %>%
  na.omit()
pecent_missing = raw_data %>%
  summarize(across(numericColumns,function(x) round((sum(is.na(x))/nrow(raw_data))*100,2))) %>%
  pivot_longer(everything(),names_to = "Variable", values_to = "Missing Number") %>%
  na.omit()
display = inner_join(mean_df,sd_df) %>%
  inner_join(median_df) %>%
  inner_join(pecent_missing)
```

## Specific Aims 2:To recommend on whether the client should move forward with the campaign and provide suggestions on campaign strategies.


### 2.1.  Graphic descriptions of the remaining seven variables related to campaign in the second study, including histograms and density plots.

```{r}
#First, we want to make qqnorm plots for all of the continuous variables, which could be used to supplement density plots and justify transformations (include in the appendix)
library(car)
dat.cont <- raw_data[numericColumns]
cont.labels <- colnames(dat.cont)
newfun <- function(x, y) qqPlot(x, main = paste(y, "QQ Plot", sep = " "))
par(mfrow = c(2, 3))
mapply(newfun, dat.cont, cont.labels)

#Analysis of which variables have censored Values
incomplete.cases <- function(x){as.numeric(count(dat.cont))-sum(complete.cases(dat.cont[x]))}
lapply(seq(1, 8, 1), incomplete.cases)

# Now let's make Densiy Plots for the variables
library(reshape2)
melt.dat.cont<- melt(dat.cont)
ggplot(data = melt.dat.cont , aes(x = value)) + stat_density() + facet_wrap(~variable, scales = "free")
```

Based on the density plots, balance, day, duration, previous, campaign, pdays.

## Necessary Transformations
```{r Transformations}
logduration <- log(raw_data$duration)
logprevious <- log(raw_data$previous+1)
logcampaign <- log(raw_data$campaign)
logday <- log(raw_data$day)  # we do not think that this will be an important variable, therefore we will not worry about transforming this
logpdays <- log(raw_data$pdays + 1.1)
testdat <- data.frame(logcampaign, logprevious, logduration, logday, logpdays)

melt.testdat <- melt(testdat)
ggplot(data = melt.testdat , aes(x = value)) + stat_density() + facet_wrap(~variable, scales = "free")
# We could not do this for balance due to a high volume of negative values
#This helped us for duration.
dat.cont$logduration <- logduration


# Let's take a look at campaign
ggplot(data=raw_data, aes(x=campaign)) + geom_density()
summary(raw_data$campaign)
outliers <- 50:65
raw_data[raw_data$campaign %in% outliers, "campaign"] = NA


# We can try Box Cox
bc = powerTransform(cbind(previous +.1, campaign, pdays+1.1 , day) ~ 1, raw_data)
summary(bc)
library(BBmisc)


ggplot(data=NULL, aes(x=campaign2)) + geom_density()
```

## 2.2.  A  scatter  plot  matrix  of  all  the  continuous  variables  in  the  secondstudy will be generated to examine association amongst variables forany necessary transformation.

```{r sactterplotmatrix}
# Scatterplot Matrix
pairs.dat <- dat.cont[c(1,5,8,9)]
pairs.dat$y <- factor(raw_data$y)
panel.cor <- function(x, y, ...)
{
par(usr = c(0, 1, 0, 1))
txt <- as.character(format(cor(as.numeric(x), as.numeric(y)), digits=2))
text(0.5, 0.5, txt, cex =1)
}
names.vector <- colnames(dat.cont)
pairs(na.omit(pairs.dat[1:5]), upper.panel=panel.cor,labels = names.vector, col=pairs.dat$y)
```

Based on the scatterplot matrix, we do not see any significant collinearity.


## 2.3  Build a logistic regression model with the binary response variable ifrespondents is willing to purchase the product, and include all of theother variables as explanatory variables.  Perform model selection to choose the optimal model.
```{r logisticregressionmodels}
# Divide the data in two, by random sampling of rows (without bootstrapping)
set.seed(333)
select.rows <- sample(1:nrow(raw_data), replace=FALSE,
                      size=floor(nrow(raw_data)/2))
# That half of the data set will now be used for selection
train.set <- raw_data[select.rows,]
# The other half will be used for inference later
test.set <- raw_data[-select.rows,]

# Fit Multiple Regression Models:
# Do not use campaign, build a test and train set
glm1 <- glm(factor(as.numeric(y))~age +  education +  log(duration) + campaign, data=train.set, family=binomial)


# Function to find RMSE
RMSE <- function(error) {sqrt(mean(error^2))}
(RMSE.all.mods <- lapply(error.all.mods, RMSE))

```

## 2.4.  In order to determine whether the campaign should move forward,estimate  the  overall  positive  response  rate  and  compare  it  to  the break-even response rate of 24.13%.

## 2.5 Identify potential high-revenue demographic groups that theclient should focus the campaign on as well as providing suggestionson targeted marketing strategies.